# MNIST Training Guide - 6-8-64 Architecture

## ğŸ“‹ æ¨¡å‹æ¶æ„

å½“å‰HLSå®ç°ä½¿ç”¨ä»¥ä¸‹æ¶æ„ï¼ˆå®šä¹‰åœ¨`src/cnn_marco.h`ï¼‰ï¼š

```
è¾“å…¥: 1Ã—28Ã—28 (MNISTç°åº¦å›¾åƒ)
  â†“
Conv1: 6ä¸ª5Ã—5å·ç§¯æ ¸ â†’ 6Ã—24Ã—24
  BatchNorm + ReLU + Quantize
  â†“
MaxPool 2Ã—2 â†’ 6Ã—12Ã—12
  â†“
Conv2: 8ä¸ª5Ã—5Ã—6å·ç§¯æ ¸ â†’ 8Ã—8Ã—8
  BatchNorm + ReLU + Quantize
  â†“
MaxPool 2Ã—2 â†’ 8Ã—4Ã—4
  â†“
Flatten â†’ 128 (8Ã—4Ã—4)
  â†“
FC1: 128â†’64 + ReLU + Dropout
  BatchNorm + Quantize
  â†“
FC2: 64â†’10 (è¾“å‡ºlogits)
  Quantize
```

**æ€»å‚æ•°**: ~10,000  
**ç›®æ ‡ç²¾åº¦**: 90-93%  
**æ•°æ®ç±»å‹**: ap_fixed<16,8> (16ä½å®šç‚¹ï¼Œ8æ•´æ•°ä½ï¼Œ8å°æ•°ä½)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸‹è½½MNISTæ•°æ®é›†

```bash
make mnist_download
```

è¿™å°†ä¸‹è½½å¹¶å¤„ç†MNISTæ•°æ®é›†åˆ°`tests/mnist/data/`ç›®å½•ã€‚

### 2. è®­ç»ƒæ¨¡å‹

**æ ‡å‡†è®­ç»ƒ** (60 epochs, ~40åˆ†é’Ÿ):
```bash
make mnist_train
```

**å¿«é€Ÿè®­ç»ƒ** (20 epochs, ~15åˆ†é’Ÿ, ç”¨äºæµ‹è¯•):
```bash
make mnist_train_quick
```

### 3. éªŒè¯æƒé‡å¯¼å‡º

è®­ç»ƒå®Œæˆåï¼Œæƒé‡ä¼šè‡ªåŠ¨å¯¼å‡ºåˆ°`tests/mnist/weights/`ï¼š
```
weights/
â”œâ”€â”€ conv1_weights.bin  (600 bytes)
â”œâ”€â”€ conv1_bias.bin     (24 bytes)
â”œâ”€â”€ conv2_weights.bin  (4,800 bytes)
â”œâ”€â”€ conv2_bias.bin     (32 bytes)
â”œâ”€â”€ fc1_weights.bin    (32,768 bytes)
â”œâ”€â”€ fc1_bias.bin       (256 bytes)
â”œâ”€â”€ fc2_weights.bin    (2,560 bytes)
â””â”€â”€ fc2_bias.bin       (40 bytes)
```

### 4. è¿è¡Œæ¨ç†æµ‹è¯•

```bash
# å¿«é€Ÿæµ‹è¯• (10å¼ å›¾ç‰‡)
make mnist_inference_quick

# å®Œæ•´æµ‹è¯• (10,000å¼ å›¾ç‰‡)
make mnist_inference_full
```

## ğŸ¯ è®­ç»ƒå‚æ•°è¯´æ˜

### é»˜è®¤å‚æ•° (60 epochs)

```python
--epochs 60          # è®­ç»ƒè½®æ•°
--batch-size 32      # æ‰¹å¤§å° (è¾ƒå°æ›´ç¨³å®š)
--lr 0.0015          # å­¦ä¹ ç‡
--dropout 0.4        # Dropoutç‡
```

### è‡ªå®šä¹‰è®­ç»ƒ

```bash
cd tests/mnist

# æ›´é•¿è®­ç»ƒ
python3 train_model.py --epochs 80 --batch-size 32

# æ›´å¿«è®­ç»ƒ
python3 train_model.py --epochs 30 --batch-size 64 --lr 0.002

# æ— æ•°æ®å¢å¼º (æ›´å¿«ä½†ç²¾åº¦å¯èƒ½ä½)
python3 train_model.py --epochs 40 --no-augment

# ä½¿ç”¨CPU
python3 train_model.py --epochs 60 --device cpu
```

## ğŸ“Š é¢„æœŸç»“æœ

### è®­ç»ƒæ›²çº¿

```
Epoch  1/60:  Train Loss: 0.8234, Acc: 72.34%  |  Test Loss: 0.4521, Acc: 85.23%
Epoch 10/60:  Train Loss: 0.2156, Acc: 93.45%  |  Test Loss: 0.1834, Acc: 91.12%
Epoch 20/60:  Train Loss: 0.1234, Acc: 95.67%  |  Test Loss: 0.1123, Acc: 92.89%
Epoch 30/60:  Train Loss: 0.0892, Acc: 96.89%  |  Test Loss: 0.0956, Acc: 93.45%
Epoch 40/60:  Train Loss: 0.0723, Acc: 97.45%  |  Test Loss: 0.0834, Acc: 93.78%
Epoch 50/60:  Train Loss: 0.0645, Acc: 97.89%  |  Test Loss: 0.0789, Acc: 93.92%
  *** New best: 93.92% - Model saved ***
```

### æœ€ç»ˆç²¾åº¦

| è®­ç»ƒæ–¹å¼ | é¢„æœŸæµ‹è¯•ç²¾åº¦ |
|---------|-------------|
| æ ‡å‡†è®­ç»ƒ (60 epochs) | 90-93% |
| å¿«é€Ÿè®­ç»ƒ (20 epochs) | 88-91% |
| æ— æ•°æ®å¢å¼º | 87-90% |

## ğŸ” å…³é”®ç‰¹æ€§

### 1. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)

æ¨¡å‹ä½¿ç”¨`FakeQuantize`å±‚æ¨¡æ‹ŸHLSçš„ap_fixed<16,8>é‡åŒ–ï¼š
- **èŒƒå›´**: -128 åˆ° 127.996
- **ç²¾åº¦**: 1/256 (çº¦0.004)
- **å¥½å¤„**: Pythonå’ŒHLSç²¾åº¦å·®å¼‚<1%

### 2. BatchNormèåˆ

è®­ç»ƒæ—¶ä½¿ç”¨BatchNormåŠ é€Ÿæ”¶æ•›ï¼Œå¯¼å‡ºæ—¶è‡ªåŠ¨èåˆåˆ°å·ç§¯/å…¨è¿æ¥å±‚æƒé‡ä¸­ï¼š
```python
# è®­ç»ƒæ—¶
Conv â†’ BN â†’ ReLU

# HLSéƒ¨ç½²æ—¶
Conv(èåˆBNçš„æƒé‡) â†’ ReLU
```

**ä¼˜åŠ¿**: é›¶é¢å¤–è®¡ç®—å¼€é”€ï¼

### 3. æ•°æ®å¢å¼º

é»˜è®¤å¯ç”¨éšæœºå¹³ç§»Â±2åƒç´ ï¼š
- è®­ç»ƒé›†ä»60,000å¼ â†’120,000å¼ 
- æå‡æ³›åŒ–èƒ½åŠ›
- ç²¾åº¦æå‡2-3%

### 4. è®­ç»ƒæŠ€å·§

- **Dropout**: 0.4 é˜²æ­¢è¿‡æ‹Ÿåˆ
- **Label Smoothing**: 0.15 æå‡æ³›åŒ–
- **Weight Decay**: 0.0002 L2æ­£åˆ™åŒ–
- **Cosine Annealing**: å­¦ä¹ ç‡å¹³æ»‘è¡°å‡
- **Gradient Clipping**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **Early Stopping**: 15è½®æ— æ”¹è¿›è‡ªåŠ¨åœæ­¢

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜1: PyTorchæœªå®‰è£…

```
ERROR: PyTorch not installed
```

**è§£å†³**:
```bash
pip3 install torch torchvision
```

### é—®é¢˜2: MNISTæ•°æ®æœªæ‰¾åˆ°

```
ERROR: MNIST data not found
```

**è§£å†³**:
```bash
make mnist_download
```

### é—®é¢˜3: ç²¾åº¦å¤ªä½ (<85%)

**å¯èƒ½åŸå› **:
- Epochå¤ªå°‘
- å­¦ä¹ ç‡ä¸åˆé€‚
- æ•°æ®æœªæ­£ç¡®å½’ä¸€åŒ–

**è§£å†³**:
```bash
# å¢åŠ epoch
python3 train_model.py --epochs 80

# è°ƒæ•´å­¦ä¹ ç‡
python3 train_model.py --lr 0.001

# æ£€æŸ¥æ•°æ®
python3 -c "import numpy as np; d=np.fromfile('data/train_images.bin', dtype=np.float32); print(f'Range: {d.min():.3f} to {d.max():.3f}')"
# åº”è¯¥è¾“å‡º: Range: 0.000 to 1.000
```

### é—®é¢˜4: è®­ç»ƒ/æµ‹è¯•ç²¾åº¦å·®è·å¤§ (>5%)

**ç—‡çŠ¶**: è¿‡æ‹Ÿåˆ
```
Train Acc: 98%
Test Acc: 88%  # å·®è·10%
```

**è§£å†³**:
```bash
# å¢åŠ Dropout
python3 train_model.py --dropout 0.5

# å¢åŠ Weight Decay (éœ€ä¿®æ”¹ä»£ç )
# æˆ–å‡å°‘è®­ç»ƒè½®æ•°
python3 train_model.py --epochs 40
```

## ğŸ“ˆ ä¸HLSé›†æˆ

### 1. éªŒè¯æƒé‡æ ¼å¼

```bash
cd tests/mnist/weights
ls -lh *.bin

# é¢„æœŸè¾“å‡º (æ–‡ä»¶å¤§å°)
# conv1_weights.bin: 600 B   (6Ã—1Ã—5Ã—5Ã—4)
# conv1_bias.bin:    24 B    (6Ã—4)
# conv2_weights.bin: 4.7K    (8Ã—6Ã—5Ã—5Ã—4)
# conv2_bias.bin:    32 B    (8Ã—4)
# fc1_weights.bin:   32K     (64Ã—128Ã—4)
# fc1_bias.bin:      256 B   (64Ã—4)
# fc2_weights.bin:   2.5K    (10Ã—64Ã—4)
# fc2_bias.bin:      40 B    (10Ã—4)
```

### 2. HLS Cä»¿çœŸ

```bash
make hls_csim
```

è¿™å°†ä½¿ç”¨å¯¼å‡ºçš„æƒé‡åœ¨HLSä¸­è¿è¡ŒCä»¿çœŸã€‚

### 3. HLSç»¼åˆ

```bash
make hls_synth
```

æ£€æŸ¥èµ„æºä½¿ç”¨ï¼š
```
Target: Zynq 7020 (xc7z020clg400-1)
LUT:  ~42,000 / 53,200 (79%)  âœ“
FF:   ~40,000 / 106,400 (38%) âœ“
DSP:  ~90 / 220 (41%)         âœ“
BRAM: ~60 / 280 (21%)         âœ“
```

### 4. ç²¾åº¦éªŒè¯

Pythonè®­ç»ƒç²¾åº¦å’ŒHLSæ¨ç†ç²¾åº¦åº”è¯¥éå¸¸æ¥è¿‘ï¼š
```
Pythonæ¨¡å‹æµ‹è¯•ç²¾åº¦: 93.45%
HLS Cä»¿çœŸç²¾åº¦:      93.12%
å·®å¼‚:               0.33%  âœ“ (< 1%å¯æ¥å—)
```

## ğŸ“ æ–‡ä»¶è¯´æ˜

### è®­ç»ƒç›¸å…³
- `train_model.py` - ä¸»è®­ç»ƒè„šæœ¬ (ä¸HLSæ¶æ„å®Œå…¨åŒ¹é…)
- `download_mnist.py` - MNISTæ•°æ®ä¸‹è½½è„šæœ¬
- `best_model.pth` - æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹

### æ¨ç†ç›¸å…³
- `mnist_inference.cpp` - HLSæ¨ç†æµ‹è¯• (ä½¿ç”¨è®­ç»ƒæƒé‡)
- `mnist_test.cpp` - HLSæµ‹è¯• (ä½¿ç”¨éšæœºæƒé‡)

### æ•°æ®ç›®å½•
- `data/` - MNISTæ•°æ®é›†äºŒè¿›åˆ¶æ–‡ä»¶
- `weights/` - å¯¼å‡ºçš„æƒé‡æ–‡ä»¶

### åºŸå¼ƒæ–‡ä»¶ (å¯åˆ é™¤)
```bash
make clean_old_scripts
```

è¿™å°†åˆ é™¤ï¼š
- `train_mnist.py`
- `train_mnist_optimized.py`
- `train_improved.py`
- `train_ultra_optimized.py`
- `train_optimized.sh`
- `train_improved.sh`

## ğŸ“ è¿›é˜¶ä¼˜åŒ–

### æå‡ç²¾åº¦åˆ°95%+

å¦‚æœéœ€è¦æ›´é«˜ç²¾åº¦ï¼ˆä»£ä»·æ˜¯æ›´å¤šèµ„æºï¼‰ï¼š

1. **å¢åŠ é€šé“æ•°** (ä¿®æ”¹`src/cnn_marco.h`):
```cpp
#define CONV2_OUT_CH 10  // ä»8å¢åˆ°10
#define FC1_OUT_SIZE 80  // ä»64å¢åˆ°80
```

2. **é‡æ–°è®­ç»ƒ**:
```bash
python3 train_model.py --epochs 80
```

3. **é‡æ–°ç»¼åˆ**:
```bash
make hls_synth
# æ£€æŸ¥LUTæ˜¯å¦è¶…é™
```

### å‡å°æ¨¡å‹ (å¦‚æœLUTä»è¶…é™)

1. **å‡å°é€šé“æ•°**:
```cpp
#define CONV1_OUT_CH 4   // ä»6å‡åˆ°4 (ä¼šæ˜¾è‘—é™ä½ç²¾åº¦!)
#define CONV2_OUT_CH 6   // ä»8å‡åˆ°6
```

2. **å¢åŠ Pipeline II**:
```cpp
// åœ¨ src/hls_cnn.h ä¸­
#pragma HLS PIPELINE II = 16  // ä»8å¢åˆ°16
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [HLS CNNé¡¹ç›®README](../../README.md)
- [HLSæ¶æ„å®šä¹‰](../../src/cnn_marco.h)
- [Vitis HLSæ–‡æ¡£](https://docs.xilinx.com/r/en-US/ug1399-vitis-hls)

---

**æœ€åæ›´æ–°**: 2025-10-04  
**æ¶æ„ç‰ˆæœ¬**: 6-8-64 (æœ€ç»ˆä¼˜åŒ–ç‰ˆ)
